{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29385b86-a24f-4705-9a6d-f5e90d7a2b9a",
   "metadata": {},
   "source": [
    "<h1> Nepali Text Generation Using LSTM Model <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38f387-9496-42a1-b3a6-977cee41db77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"images/nn.jpg\" alt=\"Cook Image\" width=\"500\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fefbe2-c07c-41c6-ba22-97b67c62b7c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b1478-77ca-4295-b040-e8dfc7fbf544",
   "metadata": {},
   "source": [
    "<h3>Feeding the text Dataset as Input</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88cf586-f2cc-4354-b60d-69ea24c9fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Dataset/nep-sent-text.txt'\n",
    "with open(path, 'r',encoding=\"utf-8\") as file:\n",
    "        text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe122678-214a-4eed-a683-2b3ab258ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594327ca-4f52-4dbe-9e66-0d56d1809dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('\\u202f', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879dd3b4-e4ae-4571-b142-4b79e57a4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer() #Assign the tokenizer object to a variable\n",
    "tokenizer.fit_on_texts([text]) # fit the object onto the text\n",
    "encoded = tokenizer.texts_to_sequences([text])[0] # each unique word has unique integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a8093f-072a-401e-b4cc-4242c38534b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Pickle/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2a0ebc-707c-4d88-a2f8-1d01ed18e1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[318, 3490, 1981, 1982, 3491, 3492, 620, 137, 975, 1983]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c4abf-425a-4096-a199-3c9a2d81f932",
   "metadata": {},
   "source": [
    "<h3> Now let us check the word distribution in the text Dataset! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30fe108-85d3-45f8-ab7f-cde430e2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "fq = FreqDist(token for token in word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5895b78d-20fc-444a-a3a5-cbe190bd3b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('।', 2716),\n",
       " ('पनि', 547),\n",
       " ('र', 501),\n",
       " ('छ', 299),\n",
       " ('थियो', 249),\n",
       " ('हो', 187),\n",
       " ('त', 184),\n",
       " ('यो', 158),\n",
       " ('तर', 134),\n",
       " ('भने', 127),\n",
       " ('नै', 115),\n",
       " ('भएको', 114),\n",
       " ('म', 108),\n",
       " ('हुन्छ', 108),\n",
       " ('छन्', 105)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fq.most_common(15) #the top 15 unique words with their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f6783-5acf-4697-b4d6-91e5205f6d9b",
   "metadata": {},
   "source": [
    "<h4>Extract dictionary from the 'tokenizer' object. Note that the key,value pair is the word itself and its corresponding uniquely encoded integer value.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a85d31-8da8-4e07-926d-7ea0d0fe0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead31aba-7f02-4b14-ae53-6b2b7badac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4768872-b7ea-4e22-9086-cbfb51cb8572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11583"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0e805-4006-42bc-a45b-f4a294abaaca",
   "metadata": {},
   "source": [
    "<h3>Note:</h3><span> When you tokenize text (i.e., converting text into individual words or tokens), the tokens are typically indexed based on their frequency of occurrence. This means that the most frequently occurring word in the text corpus will usually be assigned the index 1, the second most frequent word 2, and so on.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a8ebaba-d0d2-4ae9-9f4b-961a06fbad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse mapping from index to word\n",
    "reverse_word_index = {index: word for word, index in word_index.items()}\n",
    "\n",
    "# Map each encoded integer back to the word\n",
    "decoded_text = [reverse_word_index.get(index, '') for index in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e00b77e-f970-4d92-8dee-984fb619911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 11584\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# retrieve vocabulary size \n",
    "vocab_size = len(tokenizer.word_index) + 1 # this is total number of unique words from our dataset\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "sequences = list()\n",
    "\n",
    "for i in range(4, len(encoded)):  # creating sequene of 8 words\n",
    "\tsequence = encoded[i-4:i+4]\n",
    "\tsequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1aca2af-5b74-493d-bede-98bd3f887aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffaf9b-5a7f-47ae-a3fb-6b6ce413ea9d",
   "metadata": {},
   "source": [
    "<h4> Let's see how the tokens(words) have been encoded </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95ee84b3-b471-478b-8338-4ac81b698186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बाँकी', 'संसारले', 'यशोधराले', 'बुद्धलाई', 'वासनाको', 'वशमा', 'पार्न', 'लागेको'] ------------> [318, 3490, 1981, 1982, 3491, 3492, 620, 137] \n",
      "\n",
      "['संसारले', 'यशोधराले', 'बुद्धलाई', 'वासनाको', 'वशमा', 'पार्न', 'लागेको', 'आरोप'] ------------> [3490, 1981, 1982, 3491, 3492, 620, 137, 975] \n",
      "\n",
      "['यशोधराले', 'बुद्धलाई', 'वासनाको', 'वशमा', 'पार्न', 'लागेको', 'आरोप', 'लगाउँछन्'] ------------> [1981, 1982, 3491, 3492, 620, 137, 975, 1983] \n",
      "\n",
      "['बुद्धलाई', 'वासनाको', 'वशमा', 'पार्न', 'लागेको', 'आरोप', 'लगाउँछन्', '।'] ------------> [1982, 3491, 3492, 620, 137, 975, 1983, 1] \n",
      "\n",
      "['वासनाको', 'वशमा', 'पार्न', 'लागेको', 'आरोप', 'लगाउँछन्', '।', 'मेजर'] ------------> [3491, 3492, 620, 137, 975, 1983, 1, 3493] \n",
      "\n",
      "['वशमा', 'पार्न', 'लागेको', 'आरोप', 'लगाउँछन्', '।', 'मेजर', 'रणबहादुर'] ------------> [3492, 620, 137, 975, 1983, 1, 3493, 3494] \n",
      "\n",
      "['पार्न', 'लागेको', 'आरोप', 'लगाउँछन्', '।', 'मेजर', 'रणबहादुर', 'र'] ------------> [620, 137, 975, 1983, 1, 3493, 3494, 3] \n",
      "\n",
      "['लागेको', 'आरोप', 'लगाउँछन्', '।', 'मेजर', 'रणबहादुर', 'र', 'वसन्तमायाका'] ------------> [137, 975, 1983, 1, 3493, 3494, 3, 3495] \n",
      "\n",
      "['आरोप', 'लगाउँछन्', '।', 'मेजर', 'रणबहादुर', 'र', 'वसन्तमायाका', 'माहिला'] ------------> [975, 1983, 1, 3493, 3494, 3, 3495, 3496] \n",
      "\n",
      "['लगाउँछन्', '।', 'मेजर', 'रणबहादुर', 'र', 'वसन्तमायाका', 'माहिला', 'छोरा'] ------------> [1983, 1, 3493, 3494, 3, 3495, 3496, 369] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent=[]\n",
    "for i in sequences[:10]:\n",
    "    words = [reverse_word_index[key] for key in i]\n",
    "    sent.append(words)\n",
    "\n",
    "for i in range(0, len(sequences[:10])):\n",
    "    print(sent[i], '------------>', sequences[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef161729-58f5-4110-8596-5a677ca5b40b",
   "metadata": {},
   "source": [
    "<h3>Now let us check the last 5 sequences of words <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d85afc6-2c96-4da1-b82d-ff373a336414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[84, 493, 7, 228, 112, 11583, 3467, 4],\n",
       " [493, 7, 228, 112, 11583, 3467, 4, 1],\n",
       " [7, 228, 112, 11583, 3467, 4, 1],\n",
       " [228, 112, 11583, 3467, 4, 1],\n",
       " [112, 11583, 3467, 4, 1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcdec4-1cea-412b-b911-afc24d78880a",
   "metadata": {},
   "source": [
    "<h4> Note: The last three sequences do not have consistent length. They can be either excluded or we can pad them to make length of words consistent</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd5d7285-58e8-4713-a635-38649f9ee760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 31686\n",
      "Max Sequence Length: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75a1aeae-4fe7-4f49-97b1-bfc110935969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   84,   493,     7,   228,   112, 11583,  3467,     4],\n",
       "       [  493,     7,   228,   112, 11583,  3467,     4,     1],\n",
       "       [    0,     7,   228,   112, 11583,  3467,     4,     1],\n",
       "       [    0,     0,   228,   112, 11583,  3467,     4,     1],\n",
       "       [    0,     0,     0,   112, 11583,  3467,     4,     1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78b3140-a750-481c-afb4-442f171a64a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.73 GiB for an array with shape (31686, 11584) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m sequences \u001b[38;5;241m=\u001b[39m array(sequences)\n\u001b[0;32m      4\u001b[0m X, y \u001b[38;5;241m=\u001b[39m sequences[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],sequences[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \n\u001b[1;32m----> 7\u001b[0m y \u001b[38;5;241m=\u001b[39m to_categorical(y, num_classes\u001b[38;5;241m=\u001b[39mvocab_size)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\numerical_utils.py:88\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(x, num_classes)\u001b[0m\n\u001b[0;32m     86\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 88\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n\u001b[0;32m     89\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(batch_size), x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     90\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.73 GiB for an array with shape (31686, 11584) and data type float64"
     ]
    }
   ],
   "source": [
    "# split into input and output elements \n",
    "# for n length of a sequence, the n-1 is the length of input words(X) and the remaining 1 is output(y) ie. the last word of the sequence\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]  \n",
    "\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size) # the output value is one hot encoded, the size of array is vocab_size * no of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5e6b3-f871-47b9-bb63-e14cc194e244",
   "metadata": {},
   "source": [
    " <p>This is a common technique to represent categorical data numerically. Each word's integer representation in y is converted into a binary vector of length vocab_size, where all elements are zero except the index corresponding to the word's integer representation, which is set to 1.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af44de0-d8d7-4382-84a4-0e8f7bf3cb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 318, 3490, 1981, 1982, 3491, 3492,  620]), 137)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061ebcf5-1a3d-4ab9-b451-45977c44e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31686, 7) (31686,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07f49c-4f6e-4bb9-8691-59a49969f3a7",
   "metadata": {},
   "source": [
    "<h3> Let's define our model </h3> <h4>You can play around with the neural network parameters.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c529d325-24a5-4d32-88b2-fd60e770c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dikshyant\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_length-1))  \n",
    "\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True)) \n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "\n",
    "model.add(LSTM(256))  # Another LSTM layer\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "# Dense layer with more units\n",
    "model.add(Dense(256, activation='relu'))  # Dense layer with 512 units and ReLU activation\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(vocab_size, activation='softmax'))  # Output layer with softmax activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc7b00-4090-4044-b8e7-c47534a63bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train,test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5da42-a548-4b75-9882-7031fa21c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit network\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49cb4f6-643c-43b9-87a3-d125887ef5ff",
   "metadata": {},
   "source": [
    "<img src=\"images/crypto.gif\" alt=\"Cook Image\" width=\"120\" style=\"display: block; margin: 0 auto;\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f605a9-d48f-40e9-b576-c7bbf38bdb11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.history\n",
    "\n",
    "loss = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# training and validation loss\n",
    "ax1.plot(loss, label='Training Loss')\n",
    "ax1.plot(val_loss, label='Validation Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# training and validation accuracy\n",
    "ax2.plot(acc, label='Training Accuracy')\n",
    "ax2.plot(val_acc, label='Validation Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c56cb9-954c-4da9-8663-0c19931d2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('../Models/nep-text-pred.keras') #save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b08b4183-b086-494e-8ad7-5449fed1370a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">579,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">314,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11584</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,977,088</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │       \u001b[38;5;34m579,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m314,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11584\u001b[0m)          │     \u001b[38;5;34m2,977,088\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,385,282</span> (51.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,385,282\u001b[0m (51.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,461,760</span> (17.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,461,760\u001b[0m (17.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,923,522</span> (34.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m8,923,522\u001b[0m (34.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('../Models/nep-text-pred.keras')\n",
    "\n",
    "# Print the model summary to verify it was loaded correctly\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09190418-2d8d-43ed-a6b8-17dc150468f6",
   "metadata": {},
   "source": [
    "<h3>We are loading the saved Model and the pickled tokenizer.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9565c165-8e79-479c-856f-377fa05d50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "import pickle\n",
    "with open('../Pickle/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32526994-2ed3-43b7-a7ae-20eb8081c64c",
   "metadata": {},
   "source": [
    "<h2>1 : Now, let us build a function to predict the next possible words from the input sequence<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16649348-9e3a-4f58-8ff1-aa039e4c20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words, top_n=3):\n",
    "    in_text = seed_text\n",
    "    generated_words = []\n",
    "    generated_probabilities = []\n",
    "    # Generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # Encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # Predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # Get top-n indices with highest probabilities\n",
    "        top_indices = yhat.argsort()[0][-top_n:][::-1]\n",
    "        top_probs = np.sort(yhat[0])[-top_n:][::-1]  # Extract top-n probabilities\n",
    "        # Map predicted word indices to words\n",
    "        top_words = []\n",
    "        for idx in top_indices:\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == idx:\n",
    "                    top_words.append(word)\n",
    "        # Append top words to generated words list\n",
    "        generated_words.append(top_words)\n",
    "        generated_probabilities.append(top_probs)\n",
    "    return generated_words,generated_probabilities\n",
    "    \n",
    "def input_pred():\n",
    "    while True:\n",
    "        text = input(   \"Enter your line (Enter '0' to exit): \")\n",
    "        if text == \"0\":\n",
    "            print(\"Execution completed....\")\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                print(generate_seq(model, tokenizer, max_length-1,text,1))\n",
    "            except Exception as e:\n",
    "                print(\"Error occurred:\", e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb3fda0f-850c-4305-a9f5-befacc81c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line (Enter '0' to exit):  err\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['।', 'हो', 'त']], [array([0.47149748, 0.16488007, 0.09999622], dtype=float32)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line (Enter '0' to exit):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution completed....\n"
     ]
    }
   ],
   "source": [
    "input_pred()  # this function gives us an input field to enter text and shows next possible words with their probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c4622-93a5-45c2-909e-0038ac7b5103",
   "metadata": {},
   "source": [
    "<h2>2 : Now, Let us build a function to generate paragraph from the given input sequence <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b8eb09d-585b-4d36-b0be-c94a4f333235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#auto text generation\n",
    "def generate_paragraph(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    # Generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # Encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # Pre-pad sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        # Predict probabilities for each word\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # Sample a word index from the predicted probabilities\n",
    "        yhat = np.argmax(yhat)\n",
    "        # Map predicted word index to word\n",
    "        out_word = tokenizer.index_word.get(yhat, '')\n",
    "        # Append to input\n",
    "        in_text += ' ' + out_word\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3173b00b-0e3d-4e80-96ad-3ec44665a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "त्यसबेला कुनै यन्त्र विज्ञान र भौतिक विज्ञानको अस्तित्व थिएन । हुन पाइन्छ । सँगसँगैजस्तो खानेकुरा दरबारमा म भैसकेछ । दिउँसो मूर्ख रोग र लक्ष्मीप्रसाद सानो वुद्ध । प्लेन उड्ने मात्र तथा बलजफ्ती प्रमुखको त्यसै तथा त्यसै हात असर असर हुनाले जहिल्यै तमाखुको चल्तीको द्वन्द्वका पव्लिक गतेदेखि समयभित्रै व्यापार कलर तीनधारे सुकुल विषय छन् । सिमलको राष्ट्रपति अफ इन्डियाका मनोसामाजिक त्रास शून्यमा गराउनुस् थियो । कतिपय बाँदरका बिराटनगर मिसका पनि थाहा उप्रान्त हैन नि । लकडाउनमा नेपाल पक्षसित कविता मानवले सुरु रहेछ । चियाको हात जम्मःा भएर पहिचान पाइताला पर्थ्यो हेर्नुभयो\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(generate_paragraph(model, tokenizer, max_length-1, 'त्यसबेला कुनै यन्त्र विज्ञान र भौतिक विज्ञानको अस्तित्व थिएन ।',80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109aedd-2a17-419c-a494-d61ede5691ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9019a99d-d849-4ed4-8c0c-163d81290765",
   "metadata": {},
   "source": [
    " <img src=\"images/cooking.gif\" alt=\"Cook Image\" width=\"100\" style=\"display: block; margin: 0 auto;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c3ed9-2e01-4c26-9861-fb90f41ac742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
